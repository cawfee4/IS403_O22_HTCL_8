\documentclass{ieeeojies}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\raggedbottom 
\title{Levaring Statistical and Machine Learning models to forecast stock prices of Vietnamese tech companies}

\author{\uppercase{Ninh Thien Bao}\authorrefmark{1},
    \uppercase{Dao Tien Dat\authorrefmark{2}, and Dao Minh Huy}\authorrefmark{3}}

\address[1]{Faculty of Information Systems, University of Information Technology, (e-mail: 21520621@gm.uit.edu.vn)}
\address[2]{Faculty of Information Systems, University of Information Technology, (e-mail: 21521930@gm.uit.edu.vn)}
\address[3]{Faculty of Information Systems, University of Information Technology, (e-mail: 21520912@gm.uit.edu.vn)}

\markboth
{Author \headeretal: Ninh Thien Bao, Dao Tien Dat, Dao Minh Huy}
{Author \headeretal: Ninh Thien Bao, Dao Tien Dat, Dao Minh Huy}

\begin{abstract}
    Predicting stock prices of Vietnamese tech companies presents a unique challenge due to the dynamic and rapidly evolving nature of the technology sector in Vietnam. This study focus on investigating the efficacy of statistical models and machine learning algorithms in forecasting stock prices of prominent tech firms. Leveraging historical stock market data specific to the tech sector in Vietnam, an extensive analysis of various predictive models is conducted, including SARIMAX, Gradient Boosting Regression, Dlinear, Linear Regression, ARIMA, as well as recurrent neural network (RNN) architectures such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) networks. The investigation evaluates the performance of each algorithm in terms of prediction accuracy, robustness, and computational efficiency, with a focus on predicting stock prices of key players in the Vietnamese tech industry, including FPT Corporation (FPT), Innovative Technology Development Corporation (ITD) and CMC Corporation (CMG). Additionally. The insights derived from this study contribute to advancing the understanding of effective methodologies for predicting stock prices of Vietnamese tech companies, providing valuable guidance for investors, financial analysts operating in this dynamic sector.
\end{abstract}

\begin{keywords}
    Forecast stock price, Linear regression, GRU, LSTM, RNN, ARIMA, SARIMAX, Gradient Boosting Regressor, Dlinear 
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
The stock market plays a pivotal role in the global economy, serving as a platform for companies to raise capital and for investors to allocate funds with the expectation of returns. In Vietnam, as in many emerging economies, the stock market is characterized by unique dynamics influenced by factors such as economic development, regulatory environment, and investor sentiment. Accurately predicting stock prices in such markets is essential for investors, financial analysts, and policymakers alike, as it enables informed decision-making, risk-management, and the efficient allocation of resources.

However, forecasting stock prices is a highly challenging task due to the complexity and volatility of financial markets. Traditional methods of analysis, such as fundamental analysis and technical analysis, often fall short in capturing the complex patterns and nonlinear relationships present in stock price movements. In the last few years, there has been a growing interest in the application of statistical models and machine learning techniques to address this challenge, driven by advances in computational power, data availability, and algorithmic sophistication.

This research aims to investigate the effectiveness of using statistical models and machine learning algorithms to predict stock prices of Vietnamese tech companies. By leveraging historical stock market data from Vietnam, we seek to evaluate the performance of various predictive models and assess their ability to capture the unique characteristics of the Vietnamese stock market. Specifically, we will explore the predictive power of regression models, time series analysis techniques, and advanced machine learning algorithms.

Furthermore, we will examine the impact of incorporating different types of features, including financial indicators, market sentiment, and macroeconomic factors, on the accuracy of stock price predictions. By analyzing a diverse set of features, we aim to identify the key drivers of stock price movements in the Vietnamese market and provide insights into the factors that influence investor behavior and market dynamics.

Overall, this study contributes to the growing body of research on stock price prediction in emerging markets and provides valuable insights for investors, financial analysts operating in the Vietnamese context. By leveraging advanced analytical techniques, we aim to enhance our understanding of stock market dynamics and improve the accuracy of stock price forecasts, ultimately facilitating more informed investment decisions and promoting the efficient functioning of financial markets.
\section{Related Works}
The prediction of stock prices through the use of a combination of statistical models and machine learning techniques has received a lot of interest in recent years. Pai and Lin (2005) \cite{b1} proposed a hybrid methodology combining the autoregressive integrated moving average (ARIMA) model with support vector machines (SVMs) for stock price forecasting. Their approach aimed to harness the strengths of both models, with computational tests demonstrating promising results in capturing nonlinear patterns within stock price data.

Vijh et al. (2020) \cite{b2} explored the use of Artificial Neural Networks (ANNs) and Random Forest techniques to predict the closing prices of stocks. By utilizing financial data such as Open, High, Low, and Close prices, their models exhibited efficiency in forecasting the next day's closing price for various companies across different sectors. Evaluation metrics including Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE) indicated the effectiveness of their predictive models.

Nelson, Pereira, and Oliveira (2017) \cite{b3} delved into the application of Long Short-Term Memory (LSTM) neural networks for predicting stock price movements. Their study focused on capturing the complex and dynamic nature of stock market environments by incorporating historical price data and technical analysis indicators. Through a series of experiments, they achieved promising results, with their LSTM-based model demonstrating an average accuracy of 55.9% in predicting future trends of stock prices.

These studies collectively illustrate the diverse approaches undertaken to tackle the challenge of stock price prediction, ranging from hybrid statistical and machine learning models to advanced neural network architectures.
\section{Materials}
\subsection{Dataset}
These three datasets were gathered by using the VnStock program. These datasets are time-series dataset related to historical stock prices of Vietnamese big tech company including FPT Corporation (FPT), Innovative Technology Development Corporation (ITD) and CMC Corporation (CMG) and span from March 1, 2019 to March 1, 2024. All of the datasets include the following attribute:
\begin{itemize}
    \item Time: This field represents the time at which the stock data was recorded.
    \item Open: This field represents the opening price of the stock at the given time period. The opening price is the price at which the first trade of the day occurred.
    \item High: This field represents the highest price the stock reached during the given time period.
    \item Low: This field represents the lowest price the stock reached during the given time period.
    \item Close: This field represents the closing price of the stock at the given time period. The closing price is the price at which the last trade of the day occurred.
    \item Volume: This field represents the total volume of shares traded during the given time period. Volume typically refers to the number of shares that were traded during a specific period.
    \item Ticker: This field represents the ticker symbol or stock symbol of the company whose stock data is being recorded. Ticker symbols are unique alphabetic identifiers assigned to publicly traded companies.
\end{itemize}

\subsection{Descriptive Statistics}
\begin{table}[H]
    \centering
    \caption{Descriptive Statistics for FPT, CTR, and CMG}
    \begin{tabular}{|>{\columncolor{red!20}}c|c|c|c|}
        \hline
        \rowcolor{red!20}  & FPT         & CMG         & ITD         \\ \hline
        Count              & 1313        & 1313        & 1313        \\ \hline
        Mean               & 56880.087   & 26832.39    & 10160.89   \\ \hline
        Median             & 63470       & 28380       & 10040       \\ \hline
        Mode               & 68400       & 31300       & 7560        \\ \hline
        Standard Deviation & 27473.98    & 10097.55     & 3002.802     \\ \hline
        Standard Error     & 758.209     & 278.665     & 82.8694      \\ \hline
        Sample Variance    & 754819638.968 & 101960621.57 & 9016823.3029 \\ \hline
        Kurtosis           & -0.39622  & -0.207957  & 0.865044898    \\ \hline
        Skewness           & 0.46246  & 0.49889  & 0.923718   \\ \hline
        Range              & 118810      & 52720       & 16110       \\ \hline
        Minimum            & 19190       & 10880       & 5320        \\ \hline
        25\%               & 28410       & 17810       & 7550        \\ \hline
        75\%               & 71450       & 34350       & 11650       \\ \hline
        Maximum            & 138000      & 63600       & 21430       \\ \hline
        Sum                & 74683554    & 35230929    & 13341255    \\ \hline
    \end{tabular}
\end{table}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{bibliography/Figure/FPT_Quantiles.png}
        \caption{FPT stock price's boxplot}
        \label{fig:1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{bibliography/Figure/FPT_Bar.png}
        \caption{FPT stock price's histogram}
        \label{fig:2}
    \end{minipage}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{bibliography/Figure/ITD_Quantiles.png}
        \caption{ITD stock price's boxplot}
        \label{fig:1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{bibliography/Figure/ITD_Bar.png}
        \caption{ITD stock price's histogram}
        \label{fig:2}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{bibliography/Figure/CMG_Quantiles.png}
        \caption{CMG stock price's boxplot}
        \label{fig:1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.23\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{bibliography/Figure/CMG_Bar.png}
        \caption{CMG stock price's histogram}
        \label{fig:2}
    \end{minipage}
\end{figure}
\section{Methodology}
\subsection{Linear Regression}
Simple Linear Regression estimates the relationship between a scalar response y and a single explanatory variable $x$ (also called dependent variable $x$ and independent variable $x$), given a set of data that includes observations for both of these variables for a particular sample. \cite{b5}

The basic model for multiple linear regression is:
\begin{equation}
    Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_mX_m + \mathcal{E}
\end{equation}

Where: $Y$ is dependent variable; $X_1$, … $X_m$ is the distinct independent or predictor variables; $\beta_0$ is the $y$-intercept (value of $y$ when all other parameters are set to 0); $\beta_1$, ... $\beta_m$ is the regression coefficient of the independent variable; $\mathcal{E}$ is model error.

\subsection{ARIMA}
ARIMA is the abbreviation for “Autoregressive integrated moving average”. The ARIMA model is popularly used to forecast univariate time series data. ARIMA model can handle a time series if it is stationary and has no data missing \cite{b6}. This method is used in multiple studies for forecasting.

ARIMA models are expressed in the form of ARIMA (p,d,q) \cite{b7}. All p, d, and q are non-negative numbers.

\begin{enumerate}
    \item$AR$ is Auto Regression, and p is the number of autoregressive terms \cite{b8}. The equation for AR model is:
          \begin{equation}
              Y = \mu + \varphi_1Y_{t-1} + \varphi_2Y_{t-2} + ... + \varphi_pY_{t-p} + \varepsilon_t
          \end{equation}
          which: $Y$ is the current value; $\mu$ is the constant term; $p$ is the number of orders; $\varphi$ is the auto-regression coefficient and $\varepsilon_t$ is the error
    \item MA is the Moving Average, and q is the number of terms in the moving average [5]. The equation for MA model is:
          \begin{equation}
              Y = \mu + \theta_1Y_{t-1} + \theta_2Y_{t-2} + ... + \theta_pY_{t-p} + \varepsilon_t
          \end{equation}
          which: $Y_t$ is the current value; $\mu$ is the constant term; $p$ is the number of orders; $\theta$ is the moving average coefficient and $\varepsilon_t$ is the error
    \item Last, the I part is Integrated, and d is the number of differences (order) required to make it a stationary sequence.
\end{enumerate}

After combining them, we will have the ARIMA(p, d, q) express as follow:
\begin{equation}
    \begin{aligned}
        \Delta Y_t = \mu + \varphi_1Y_{t-1} + \varphi_2Y_{t-2} + ... +\varphi_pY_{t-p} \\
        + \theta_1Y_{t-1} + \theta_2Y_{t-2} + ... + \theta_pY_{t-p} + \varepsilon_t
    \end{aligned}
\end{equation}

\subsection{SARIMAX}
Seasonal ARIMAX is an advancement of the seasonal ARIMA with external feature variables (X) called SARIMAX ($p, d, q$) ($P,D,Q$)s ($X$) to improve its prediction and performance \cite{b9}. Where $X$ is the vector of external variables; where the small letter parentheses part ($p,d,q$) indicates the non-seasonal part of model while the capital letter part ($P,D,Q$)[$s$] indicates the seasonal part of model, $s$ being the number of periods per season \cite{b4}. The external variables can be modeled by multi linear regression equation is expressed as equation (1). The general seasonal autoregressive integrated moving average (SARIMA) model written as follows:
\begin{equation}
    \Phi_P(B^S)\phi_p(B)\bigtriangledown_S^D\bigtriangledown^d = \theta_q(B)\Theta_Q(B^S)\varepsilon_t
\end{equation}
where $\Phi_P(B^S) = (1 - \Phi_1B^S - ... - \Phi_pB^{sP})$ is the seasonal AR operator of order P;\\
$\phi_p(B) = (1 - \phi_1B - ... - \phi_pB^p)$ is the regular AR operator of order p; $\bigtriangledown_S^D = (1 - B)^d$ represents the seasonal differences and $\bigtriangledown_S^D = (1 - B^S)^D$ the regular differences;\\
$\Theta_Q(B^S) = (1 - \Theta_1B^S - ... - \Theta_QB^{sQ})$ is the seasonal moving average operator of order Q; \\
$\theta_q(B) = (1 - \theta_1B - ... - \theta_qB^q) $ is the regular moving average operator of order q;\\
$\varepsilon_t$ is a white noise process.

SARIMAX model demonstrates the use of exogenous variables by using the concept of “regression with SARIMA errors”. The model can be written as:
\begin{equation}
    y_t = \beta x_t + z_t
\end{equation}

This equation is just a linear regression to depict the linear effect of exogenous variables on $y_t$. The error term $z_t$ follows the SARIMA process and can be described by usual SARIMA equation as
\begin{equation}
    \Phi_P(B^S)\phi_p(B)\bigtriangledown_S^D\bigtriangledown^dz_t = \theta_q(B)\Theta_Q(B^S)\varepsilon_t
\end{equation}

where all the notations and operators have same meaning as above \cite{b4}.
\subsection{Recurrent Natural Network}
Recurrent Natural Networks is a type of neural network that can be used to model sequence data. RNNs, which are formed from feedforward networks, are similar to human brains in their behaviour. Simply said, recurrent neural networks can anticipate sequential data in a way that other algorithms can’t \cite{b10}. RNN processes sequence data by connecting previous outputs to current calculations, unlike traditional neural networks. It memorizes past information, incorporating it into current outputs, allowing for processing sequences of any length.
\begin{figure} [H]
    \centering
    \includegraphics[width=0.5\textwidth]{bibliography/Figure/RNN_overview.png}
    \caption{Overview the Recurrent Natural Network}
    \label{fig:rnn-overview}
\end{figure}

The value $h$ of the hidden layer of RNN not only depends on the current input $x$, but also depends on the value $h$ of the last hidden layer, as shown in figure 8:
\begin{figure} [H]
    \centering
    \includegraphics[width=0.5\textwidth]{bibliography/Figure/What-is-Recurrent-Neural-Network.jpg}
    \caption{RNN hidden layer calculate process.}
    \label{fig:rnn-hidden-layer}
\end{figure}
Among them, $t$ is the time, $x$ is the input layer, $h$ is the hidden layer, $L$ is the output layer, and the matrix $W$ is the last value of the hidden layer as the weight of this input. RNN training also uses the BP error back propagation algorithm, but there is a little difference. In the training process, the parameters $W, U, V$ are shared, but the traditional fully connected neural network is not. And in using the gradient descent algorithm, the output of each step not only depends on the current step of the network, but also depends on the state of the previous steps of the network.

\subsection{Long Short Term Memory}
Long short term memory (LSTM) \cite{b12} is a deformation structure of RNN by adding memory cell into hidden layer, so as to control the memory information of the time series data. Information is transmitted among different cells of hidden layer through several controllable gates (forget gate, input gate, output gate) \cite{b13}, thus the memory and forgetting extent of the previous and current information can be controlled. Compared with traditional RNN, the LSTM has the long term memory function and its gradient disappearance problem can be avoided. Two gates of LSTM are designed for controlling the state of memory cell, one is forget gate which indicates how much ‘‘memory’’ of the last moment’s cell can be saved, the other is input gate, which determines how much input of the current moment can be saved to the cell state, and controls the proportion of fusion of ‘‘historical’’ information and ‘‘current’’ stimulus. Finally, the output gate of LSTM is designed for controlling how much information is output for cell status.
\begin{figure} [H]
    \centering
    \includegraphics[width=0.5\textwidth]{bibliography/Figure/LSTM _structure.png}
    \caption{LSTM Structure network}
    \label{fig:LSTM structure}
\end{figure}

In Figure 9, $\sigma$ is the sigmoid function shown in equation (13), whose output is a value between 0 and 1. Here, 0 means ‘‘let nothing pass’’ while 1 means "let everything pass". Then the hyperbolic tangent function illustrated in Equation (14), is used to overcome the problem of gradient disappearance.
\begin{equation}
    f_t = \sigma(W_f.[h_{t-1}, X_t] + b_f)
\end{equation}
\begin{equation}
    i_t = \sigma(W_i.[h_{t-1}, X_t] + b_i)
\end{equation}
\begin{equation}
    \hat{C}_t = tanh(W_c.[h_{t-1}, X_t] + b_c)
\end{equation}
\begin{equation}
    C_t = f_t*C_{t-1} + i_t + \hat{C}_t
\end{equation}
\begin{equation}
    O_t = \sigma(W_o.[h_{t-1}, X_t] + b_o)
\end{equation}
\begin{equation}
    sigmoid(x) = \frac{1}{1+e^{-x}}
\end{equation}
\begin{equation}
    tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{equation}

where $W_f, W_i, W_c$ and $W_o$ are input weights, $b_f, b_i, b_c$ and $b_o$ are bias weights, $t$ represents the current time state, and $t - 1$ is the previous time state, $X$ represents input; $H$ represents output and $C$ is the cell status.

\subsection{Gate Recurrent Unit}
GRU is one of the most popular improved variants of RNN with a special gated recurrent neural network based on optimized LSTM. The GRU internal structure is similar to the internal structure of the LSTM, except that the GRU associates the input gate and the forget gate in the LSTM unit into a single update gate. This model has two gates: one is the update gate, which controls the extent and retains previous information in the current state; the other represents the reset gate which determines whether the previous information and the current state are to be associated \cite{b14}. Figure 10 shows the basic design of a GRU unit.
\begin{figure} [H]
    \centering
    \includegraphics[width=0.5\textwidth]{bibliography/Figure/GRU.png}
    \caption{The structure of GRU units}
    \label{fig:GRU structure}
\end{figure}
According to Figure 10, the formulas of GRU can be given as:
\begin{equation}
    z_t = \sigma(w_z[h_{t - 1}, X_t] + b_z)
\end{equation}
\begin{equation}
    r_t = \sigma(w_r[h_{t - 1}, X_t] + b_r)
\end{equation}
\begin{equation}
    a_t = tanh(r_t * w_a[h_{t - 1}, X_t] + b_a)
\end{equation}
\begin{equation}
    h_t = (1 - z_t) * a_t + z_t * h_{t - 1}
\end{equation}
where $X_t$ is the vector input of training data at time $t$, and $h_t$ is the outcome of the current layer at time $t$. $z_t$ and $r_t$ represent the update and the reset gates respectively at is the candidate activation.

\subsection{Gradient Boosting Regressor}
A gradient boosting regressor model involves an ensemble learning approach where robust forecasting models are formed by integrating several individual regression trees (decision trees) that are referred to as weak learners. Such an algorithm reduces the error rate of weakly learned models (regressors or classifiers). Weakly learned models are those which have a high bias regarding the training dataset, with low variance and regularization, and whose outputs are considered only somewhat improved when compared with arbitrary guesses \cite{b11}. Generally, boosting algorithms contains three components, namely, an additive model, weak learners, and a loss function.

GBM (gradient boosting machines) operate by identifying the limitations of weak models via gradients. This is attained with the help of an iterative approach, where the task is to finally join base learners to decrease forecast errors, where decision trees are combined by means of an additive model while reducing the loss function via gradient descent. The GBT (gradient boosting tree) \( F_n(x_t) \) can be defined as the summation of n regression-trees.

\begin{equation}
    F_n(x_t) = \sum_{i=1}^{n} f_i(x_t)
\end{equation}

where every \(F_i(x_t)\) is a decision tree (regression-tree). The ensemble of trees is constructed
sequentially by estimating the new decision tree fn+1(xt) with the help of the following
equation:
\begin{equation}
    \arg\min_{\{t\}} \sum_{t} L\left(y_t, F_n(x_t) + f_{n+1}(x_t)\right)
\end{equation}
This equation represents the process of finding the next weak learner, denoted as $f_{n+1}(x_t)$, that minimizes the loss function $L$ when added to the current ensemble predictor $F_n(x_t)$, given the training data $(x_t, y_t)$. Here, $L$ measures the discrepancy between the true target values $y_t$ and the predictions made by the current ensemble.

The process iterates, adding weak learners one at a time, each one addressing the residuals or errors left by the previous ensemble. This sequential construction continues until a stopping criterion is met, such as reaching a predefined number of trees or when further adding trees no longer improves performance on a validation set.

In summary, gradient boosting regression iteratively improves a predictive model by combining multiple weak learners into a strong predictor, with each new learner focusing on the mistakes of the previous ensemble.
\subsection{DLinear}
DLinear, or Decomposition Linear, is a model introduced for time series forecasting.  It first decomposes a raw data input into a trend component by a moving average kernel and a remainder (seasonal) component. Then, two
one-layer linear layers are applied to each component,
and we sum up the two features to get the final prediction. By explicitly handling trend, DLinear enhances
the performance of a vanilla linear when there is a clear
trend in the data \cite{b15}.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.5\textwidth]{bibliography/Figure/Illu_DLinear.png}
    \caption{Illustration of the Decomposition Linear Model}
    \label{fig:DLinear Illustration}
\end{figure}

The overall structure of DLinear is shown in Figure 11(a). The whole process is:

\subsubsection{Decomposition}
DLinear decomposes the time series $\mathbf{x} \in \mathbb{R}^{L \times C}$ into trend and seasonal components:

\begin{equation}
    \mathbf{x}(t) = \mathbf{T}(t) + \mathbf{S}(t) + \epsilon(t),
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{T}(t)$ represents the trend component,
    \item $\mathbf{S}(t)$ represents the seasonal component,
    \item $\epsilon(t)$ is the residual (noise) component.
\end{itemize}

\subsubsection{Trend Extraction}
The trend component $\mathbf{T}(t)$ captures the long-term progression of the time series. It can be extracted using methods such as moving averages or polynomial fitting:

\begin{equation}
    \mathbf{T}(t) = \frac{1}{2k+1} \sum_{i=-k}^{k} \mathbf{x}(t+i),
\end{equation}

where $k$ is the window size for the moving average.

\subsubsection{Seasonal Component}
The seasonal component $\mathbf{S}(t)$ captures the repeating patterns within the time series. It can be obtained by removing the trend from the original series:
\begin{equation}
    \mathbf{S}(t) = \mathbf{x}(t) - \mathbf{T}(t).
\end{equation}

\subsubsection{Linear Modeling}
Separate linear models are used to predict the trend and seasonal components:
\begin{align}
    \hat{\mathbf{T}}(t+h) & = \mathbf{W}_T \mathbf{T}(t) + \mathbf{b}_T \\
    \hat{\mathbf{S}}(t+h) & = \mathbf{W}_S \mathbf{S}(t) + \mathbf{b}_S
\end{align}

where:
\begin{itemize}
    \item $\mathbf{W}_T$ and $\mathbf{W}_S$ are weight matrices for the trend and seasonal components, respectively,
    \item $\mathbf{b}_T$ and $\mathbf{b}_S$ are bias vectors for the trend and seasonal components, respectively,
    \item $t+h$ indicates the forecasted value $h$ steps ahead from the current time $t$, as illustrated in Figure 11(b).
\end{itemize}

The final forecast \( \hat{\mathbf{x}}(t+h) \) is obtained by combining the predictions from the trend and seasonal models:
\begin{equation}
    \hat{\mathbf{x}}(t+h) = \hat{\mathbf{T}}(t+h) + \hat{\mathbf{S}}(t+h).
\end{equation}
DLinear offers an efficient and effective approach to time series forecasting by leveraging decomposition and linear transformations. This model addresses the limitations of traditional Transformers, making it suitable for long-term forecasting tasks.
\section{RESULT}
In this section, we will evaluate eight different models in forecasting: Linear Regression, ARIMA, SARIMAX, LSTM, GRU, DLinear, RNN, and Gradient Boosting Regressor. Initially, we will preprocess the original time series dataset. Following preprocessing, we will divide the data into training, testing, and validation sets using three different ratios: 60:20:20, 70:20:10, and 80:20:10. Each model will be trained on the training set and subsequently used to make predictions on the test and validation sets. The performance of these models will be assessed using the following evaluation metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). The results are presented in the tables below.
\subsection{CMG}
\begin{table}[H]
    \renewcommand{\arraystretch}{1.5}
    \centering
    \begin{tabular}{|c|c|p{1cm}|p{1cm}|p{1cm}|}
        \hline
        \textbf{Model} & \textbf{Train/Test/Val} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} \\
        \hline
        % Linear Regression
        \multirow{3}{*}{LR}
                       & 6 - 2 - 2               & 4014.54       & 3113.24      & 9.16\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 3479.93       & 2922.17      & 9.19\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 1906.99       & 1597.14      & 4.43\%        \\
        \hline
        % ARIMA
        \multirow{3}{*}{ARIMA}
                       & 6 - 2 - 2               & 2295.8        & 1886.6       & 5.51\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 2295.8        & 1886.6       & 5.51\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 2295.8        & 1886.6       & 5.51\%        \\
        \hline
        % SARIMAX
        \multirow{3}{*}{SARIMAX}
                       & 6 - 2 - 2               & 1217.97       & 1098.21      & 3.42\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 649.34        & 507.62       & 1.53\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 1035.35       & 850.25       & 2.28\%        \\
        \hline
        % RNN
        \multirow{3}{*}{RNN}
                       & 6 - 2 - 2               & 1323.78       & 1019.18      & 3.13\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 766.84        & 555.26       & 1.65\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 773.26        & 569.71       & 1.55\%        \\
        \hline
        % GRU
        \multirow{3}{*}{GRU}
                       & 6 - 2 - 2               & 1208.25       & 885.54       & 2.71\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 765.72        & 557.01       & 1.66\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 1058.28       & 864.53       & 2.32\%        \\
        \hline
        % LSTM
        \multirow{3}{*}{LSTM}
                       & 6 - 2 - 2               & 1216.88       & 894.34       & 2.74\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 888.80        & 703.16       & 2.11\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 760.21        & 557.73       & 1.51\%        \\
        \hline
        % GBR
        \multirow{3}{*}{GBR}
                       & 6 - 2 - 2               & 1151.03       & 800.82       & 2.41\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 609.51        & 435.54       & 1.29\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 622.22        & 446.29       & 1.20\%        \\
        \hline
        % DLinear
        \multirow{3}{*}{DLinear}
                       & 6 - 2 - 2               & 732.98        & 577.72       & 1.76\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 731.06        & 575.41       & 1.75\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 645.57        & 475.18       & 1.46\%        \\
        \hline
    \end{tabular}
    \caption{Performance Metrics on CMG's test set}
    \label{tab:performance_metrics_cmg}
\end{table}
\subsection{FPT}
\begin{table}[H]
    \renewcommand{\arraystretch}{1.5}
    \centering
    \begin{tabular}{|c|c|p{1cm}|p{1cm}|p{1cm}|}
        \hline
        \textbf{Model} & \textbf{Train/Test/Val} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} \\
        \hline
        % Linear Regression
        \multirow{3}{*}{LR}
                       & 6 - 2 - 2               & 12021.71      & 10138.72     & 15.22\%       \\
        \cline{2-5}
                       & 7 - 2 - 1               & 12399.92      & 11311.74     & 16.21\%       \\
        \cline{2-5}
                       & 8 - 1 - 1               & 7958.24       & 6875.53      & 8.23\%        \\
        \hline
        % ARIMA
        \multirow{3}{*}{ARIMA}
                       & 6 - 2 - 2               & 9631          & 6259.7       & 7.3\%         \\
        \cline{2-5}
                       & 7 - 2 - 1               & 9631          & 6259.7       & 7.3\%         \\
        \cline{2-5}
                       & 8 - 1 - 1               & 9631          & 6259.7       & 7.3\%         \\
        \hline
        % SARIMAX
        \multirow{3}{*}{SARIMAX}
                       & 6 - 2 - 2               & 1995.53       & 1544.12      & 1.75\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 1076.95       & 698.12       & 0.91\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 1995.53       & 1544.12      & 1.75\%        \\
        \hline
        % RNN
        \multirow{3}{*}{RNN}
                       & 6 - 2 - 2               & 1804.73       & 1284.95      & 1.89\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 1572.73       & 1106.49      & 1.43\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 2793.28       & 2309.79      & 2.68\%        \\
        \hline
        % GRU
        \multirow{3}{*}{GRU}
                       & 6 - 2 - 2               & 1735          & 1184.36      & 1.74\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 1656.52       & 1142.7       & 1.47\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 2114.84       & 1555.88      & 1.78\%        \\
        \hline
        % LSTM
        \multirow{3}{*}{LSTM}
                       & 6 - 2 - 2               & 1944.89       & 1448.76      & 2.11\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 1739.26       & 1237.97      & 1.58\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 3425.63       & 2921.82      & 3.36\%        \\
        \hline
        % GBR
        \multirow{3}{*}{GBR}
                       & 6 - 2 - 2               & 2033.93       & 1433.24      & 2.06\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 9246.01       & 5228.78      & 5.82\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 13305.43      & 10220.85     & 11.1\%        \\
        \hline
        % DLinear
        \multirow{3}{*}{DLinear}
                       & 6 - 2 - 2               & 1430.42       & 1122.67      & 1.49\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 1336.28       & 1007.43      & 1.33\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 1216.28       & 891.05       & 1.17\%        \\
        \hline
    \end{tabular}
    \caption{Performance Metrics on FPT's test set}
    \label{tab:performance_metrics_fpt}
\end{table}
\subsection{ITD}
\begin{table}[H]
    \renewcommand{\arraystretch}{1.5}
    \centering
    \begin{tabular}{|c|c|p{1cm}|p{1cm}|p{1cm}|}
        \hline
        \textbf{Model} & \textbf{Train/Test/Val} & \textbf{RMSE} & \textbf{MAE} & \textbf{MAPE} \\
        \hline
        % Linear Regression
        \multirow{3}{*}{LR}
                       & 6 - 2 - 2               & 5590.21       & 5395.83      & 50.51\%       \\
        \cline{2-5}
                       & 7 - 2 - 1               & 3976.87       & 3816.96      & 34.74\%       \\
        \cline{2-5}
                       & 8 - 1 - 1               & 3144.64       & 2962.17      & 27.54\%       \\
        \hline
        % ARIMA
        \multirow{3}{*}{ARIMA}
                       & 6 - 2 - 2               & 1325.59       & 1168.96      & 10.16\%       \\
        \cline{2-5}
                       & 7 - 2 - 1               & 1325.59       & 1168.96      & 10.16\%       \\
        \cline{2-5}
                       & 8 - 1 - 1               & 1325.59       & 1168.96      & 10.16\%       \\
        \hline
        % SARIMAX
        \multirow{3}{*}{SARIMAX}
                       & 6 - 2 - 2               & 224.92        & 172.2        & 1.56\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 197.67        & 161.55       & 1.45\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 145.77        & 114.32       & 1.03\%        \\
        \hline
        % RNN
        \multirow{3}{*}{RNN}
                       & 6 - 2 - 2               & 411.11        & 303.07       & 2.77\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 327.4         & 230.8        & 2.08\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 289.91        & 212.55       & 1.97\%        \\
        \hline
        % GRU
        \multirow{3}{*}{GRU}
                       & 6 - 2 - 2               & 436.12        & 326.14       & 3.01\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 320.45        & 228.5        & 2.06\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 314.66        & 250.91       & 2.34\%        \\
        \hline
        % LSTM
        \multirow{3}{*}{LSTM}
                       & 6 - 2 - 2               & 404.67        & 291.17       & 2.67\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 318.7         & 227.92       & 2.05\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 284.67        & 219.03       & 2.05\%        \\
        \hline
        % GBR
        \multirow{3}{*}{GBR}
                       & 6 - 2 - 2               & 442.19        & 346.47       & 3.16\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 302.22        & 238.29       & 2.13\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 232.70        & 170.19       & 1.55\%        \\
        \hline
        % DLinear
        \multirow{3}{*}{DLinear}
                       & 6 - 2 - 2               & 287.85        & 217.10       & 1.96\%        \\
        \cline{2-5}
                       & 7 - 2 - 1               & 335.67        & 267.45       & 2.41\%        \\
        \cline{2-5}
                       & 8 - 1 - 1               & 273.21        & 201.84       & 1.82\%        \\
        \hline
    \end{tabular}
    \caption{Performance Metrics on ITD's test set}
    \label{tab:performance_metrics_itd}
\end{table}
\subsection{Forecasting Plot}
\begin{figure} [H]
    \centering
    \includegraphics[width=0.4\textwidth]{bibliography/Figure/LinearRegression_CMG_721_90.png}
    \caption{Linear Regression - CMG - 721}
    \label{fig:LR_CMG_721_90}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=0.4\textwidth]{bibliography/Figure/ARIMA_FPT_721_90.png}
    \caption{ARIMA - FPT - 721}
    \label{fig:ARIMA_FPT_721_90}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=0.4\textwidth]{bibliography/Figure/SARIMAX_ITD_721_90.png}
    \caption{SARIMAX - ITD - 721}
    \label{fig:SARIMAX_ITD_721_90}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=0.4\textwidth]{bibliography/Figure/RNN_CMG_721_90.png}
    \caption{RNN - CMG - 721}
    \label{fig:RNN_CMG_721_90}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=0.4\textwidth]{bibliography/Figure/GRU_FPT_721_90.png}
    \caption{GRU - FPT - 721}
    \label{fig:GRU_FPT_721_90}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=0.4\textwidth]{bibliography/Figure/LSTM_ITD_721_90.png}
    \caption{LSTM - ITD - 721}
    \label{fig:LSTM_ITD_721_90}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=0.4\textwidth]{bibliography/Figure/GBR_CMG_721_90.png}
    \caption{GBR - CMG - 721}
    \label{fig:GBR_CMG_721_90}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=0.4\textwidth]{bibliography/Figure/DLinear_FPT_721_90.png}
    \caption{DLinear - FPT - 721}
    \label{fig:DLinear_FPT_721_90}
\end{figure}
\section{Conclusion}

%% UNCOMMENT these lines below (and remove the 2 commands above) if you want to embed the bibliografy.
\begin{thebibliography}{00}
    \bibitem{b1} Pai, P.-F. and Lin, C.H. (2005) ‘A hybrid ARIMA and support vector machines model in stock price forecasting’.
    \bibitem{b2} Vijh, M. et al. (2020) ‘Stock Closing Price Prediction using Machine Learning Techniques’.
    \bibitem{b3}Nelson, D.M., Pereira, A.C.M. and Oliveira, R.A. de (2017) ‘Stock market’s price movement prediction with LSTM neural networks’.
    \bibitem{b4}S. Kumar, A. Gupta, K. Arora, and K. Vatta, “Effect of Rainfall in Predicting Tomato Prices in India: An Application of SARIMAX and NARX Model,” vol. 32, pp. 159–164, Dec. 2022.
    \bibitem{b5}M. Tranmer, J. Murphy, M. Elliot, and M. Pampaka, “Multiple Linear Regression (2nd Edition)”.
    \bibitem{b6}V. Ş. Ediger and S. Akar, “ARIMA forecasting of primary energy demand by fuel in Turkey,” Energy Policy, vol. 35, no. 3, pp. 1701–1708, Mar. 2007.
    \bibitem{b7}B. Dey, B. Roy, S. Datta, and T. S. Ustun, “Forecasting ethanol demand in India to meet future blending targets: A comparison of ARIMA and various regression models,” Energy Rep., vol. 9, pp. 411–418, Mar. 2023.
    \bibitem{b8}X. Jin and C. Yi, “The Comparison of Stock Price Prediction Based on Linear Regression Model and Machine Learning Scenarios,” presented at the 2022 International Conference on Bigdata Blockchain and Economy Management (ICBBEM 2022), Atlantis Press, Dec. 2022, pp. 837–842.
    \bibitem{b9}Manigandan P, Alam MS, Alharthi M, Khan U, Alagirisamy K, Pachiyappan D, Rehman A. Forecasting Natural Gas Production and Consumption in United States-Evidence from SARIMA and SARIMAX Models. Energies. 2021; 14(19):6021.
    \bibitem{b10}D. Kalita, “A Brief Overview of Recurrent Neural Networks (RNN),”Analytics Vidhya, Mar. 11, 2022.
    \bibitem{b11}U. Singh, M. Rizwan, M. Alaraj, and I. Alsaidan, “A Machine Learning-Based Gradient Boosting Regression Approach for Wind Power Production Forecasting: A Step towards Smart Grid Environments,” Aug. 2021.
    ‌\bibitem{b12} S. Hochreiter and J. Schmidhuber, ‘‘Long short-time memory,’’ Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997.
    \bibitem{b13}F. A. Gers, J. Schmidhuber, and F. Cummins, ‘‘Learning to forget:
    Continual prediction with LSTM,’’ Neural Comput., vol. 12, no. 10, pp. 2451–2471, Oct. 2000.
    \bibitem{b14}Mahjoub, S., Chrifi-Alaoui, L., Marhic, B., \& Delahoche, L. (2022). Predicting energy consumption using LSTM, multi-layer GRU and drop-GRU neural networks. Sensors, 22(11), 4062.
    \bibitem{b15}Zhou, T., Ma, Z., Zhou, Z., Chen, J., Wang, X., Jiang, Z., \& Jin, X. (2022).
    ‘‘Are Transformers Effective for Time Series Forecasting?’’. arXiv:2205.13504v3 [cs.AI] 17 Aug 2022.

\end{thebibliography}
%%%%%%%%%%%%%%%
\EOD

\end{document}